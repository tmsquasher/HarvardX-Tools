Development log
10/26
It would be useful to know what groups are not being categorized by the separateAllByClass.py
program. Reading through the unknown*.log for each directory can help, but is pretty
tedious. So I've written a quick script, distillUnknownLabels, that will show the event_type 
field for all of the uncategorized. This should be run from the directory above the one that
contains all of the log files, and will aggregate all of the unknown files in a single list, 
with counts for the number of events.

The program separateAllByClass.py also created files for all of the classes, not just those that
had a log entry. I made a change so that there is now a fileOut object that will only create
a file when there is a log entry to write. I also set it up so that different variations of the
name for the same class will write to the same log file.

10/25
Working on re-parsing and re-separating the log files. It isn't clear that there is
as simple naming convention that is followed both by the naming of the data files and
used somewhere in each log line to identify the log entries. This may come, but we don't
have it yet. There are even seemingly random variations, like SPU27x (for the on-line
open version of the cooking class) and SPU27X (for the edge version).

Added some courses to those that are tracked by the separateAllByClass.py script. Since
there is currently no clear way to establish a connection between the courses that are 
being offered and the log items that are written for those courses, it will need to be 
done by inspection and by hand. Also changed the log files used to construct the server-
based log files list to look for the pattern '*HarvardX.log' so the files generated by
this program won't be read as input files.

10/20
Time to do some cleanup on various files and workflows. Goal is to get all of the processing
of the class files automated once again, and perhaps to get the log files re-automated.

Log status: We currently have the logs for the various classes from 2013-01-20 to the present.
We also have the raw logs (don't seem to be separated yet) for the period from 2012--7-24 until 
2012-11-20. This leaves a two-month period between 11/20/12 and -1-20-13 missing. I recall
that there was a period when, because of a bug, the logs were not begin gathered/saved; need 
to check with the edX folks to see if this is the time period that is missing. If so, these
logs are just gone, and we will need to deal with it. Otherwise, we should try to get those
files back.

A minor change that will be helpful-- make the file generated by the attempt to build a 
list of classes start with a lower-case character, so that all directories will start with
an upper-case character and all files a lower-case character. I've also expanded the list
of prefixes that are being removed from the names of the classes.
	buildClassList.py: changed output file name to weeklyClassList, added name prefixes
		to be deleted, cleaned up the code that reduces the name.
	separateClassFiles.py: changed input file name to weeklyClassList

processClassData.sh: replace the separateFiles.sh and renameInDirectory.sh scripts with
buildClasRoster.py and separateClassFiles.py. The separateClassFiles.py also replaces 
the functionality in renameInDirectory.sh, which has been removed from the script. I added
functionality that will call the conversion program to .csv for all of the sql files that 
are sorted out, as well.

There are getting to be a lot of junk files in the dumps-- mostly demo or test classes
that have been added for practice or the like. I wrote killListedFiles.py, which currently has
a static list of patterns that, if present in a file name, will cause that file to be 
deleted early on. This replaces the shell rm commands; we should make sure that we keep
the list of patterns up to day.

10/16
Added duplicate id and username detection to globalUserList.py. If any duplicates are
detected, the duplicates are written to a dictionary and, at the end of the run, if the
dictionaries (one for ids, the other for usernames) are not empty, they are written to 
files.

Also experimented with using json as an output format for the tables being built. Using
json makes the code somewhat simpler (simply open a file and do a json.dump to the file), 
but it turns out to be slower than writing a .csv line by line. When writing the half-million+
user to id table, writing to json added about a second to the overall time of the program.
The resulting file was also larger by about 20%.

9/22
Fixed the problem with separateClassFiles.py; the cause was a break statement where
there needed to be a continue; the question now is why it worked for some rather than
why it didn't work for all. But since it is working there is little need to track that
down. 

The next step in dealing with the class data will be to determine which of the data
files are empty and not copy those to the class directory, but simply remove them. This 
has been added.

The next step will be to make sure that the log data follows the new naming conventions
that are embodied in this work, and then to change the log processing scripts to deal
with the new naming conventions.  

9/21
Coming up with a class list requires parsing the names of the files. The naming
pattern includes a (useless) prefix of "HarvardX-", followed by a description and
some timing information, followed by a description of the data in the file, ending
with "edx-analytics" and a file extension. The exception to this is the .mongo files;
these have the 'HarvardX-' prefix, a '.mongo' extension, and nothing but the class
name (if an on-line course) or a postfix prior to the extension of '-edge'. 

Current theory: to build a class list that can be used to segregate the data files
by class, use the .mongo files, strip them of the 'HarvardX-' prefix and the 
'.mongo' postfix, and work with the convention that non-edge courses have no 
additional name but that edge courses are marked as '-edge' at the end.

buildClassList.py
	Run either by itself or with a directory as a command-line argument, the
	program will read the names of the file in the directory indicated or the
	current working directory, extracting the names of classes in the files.
	The assumption is that the program will run in a directory that contains
	the class dump from edX. The results will be written, on class-per-line,
	in a file named WeeklyClassList in the working directory.
	
	The module also contains functions that will write the class list from the
	file and read the class list from a file.
	
Question: We have been using the class name to determine what log entries are
associated with the class, since the class name is part of the URL that is in the
log for each event. Is this still true, and is it the name that we are extracting?

Now that we have a list of files, we can create a directory for each file, and 
move all the files associated with a particular class to that directory. Creating
the directories is easy; deciding which directory to move a file to is a bit more
complex since the kind of contents of the file (authorized-user, userprofile, 
and the like) are embedded between the course name and the distinction between
and edX and edge in the name. 

This lead to the current separateClassFiles.py, which attempts to:
	Read the WeeklyClassList file (generated by buildClassList.py);
	Create a directory for each class;
	Finding all of the files that aren't directories that have the
	class name as part of their name;
	Determine, based on what the description of the file is embedded in
	the name, to give it a more descriptive and shorter name, and
	Move the file to the appropriate directory with the shorter (and more
	regular) name.
A current version of this script is working for some files, but not others; it isn't
clear what the differences are; this is where to start next.

9/20
Problem: The whole of the HarvardX research work has been based on an organization
of the files sent by edX into category by class, with the data from each week
living under the class-named directory and aggregation at the class level. The
conventional naming of edX for the classes has been followed.

In the 2013-09-08 data dump edX changed their convention for naming the files 
containing the data, and began to include all data for Harvard courses, including
those for experimental and on-campus courses (the edge courses) and new courses
for the semester. The file count has gone up from 397 in the 2013-09-08 dump
to 925 in the 2013-09-15 dump. In addition, the 2013-09-08 dump contained two 
files for each database extraction; one in .xml format and one in .sql format. The
2013-09-15 dump only contains the .sql format, which means that 184 files in the
earlier dump are not in the later dump.

We need a better way to process the files in the new, expanded dump. We will be
adopting the following strategy:

1) remove any files that are not in use. This includes all of the various wiki
files, which do not contain any data. This will remove 180 files. Remove all 
*Demo* files, which removes an additional 18.Remove all *00-Test* files, which
removes an additional 12. It is unclear if we can remove the *Beta-Test* files;
these have some content but appear to not have much student content. This gets
the listing down to 708 files.

2) Determine the class names from the file names. All of the files begin with 
a preface of "HarvardX-" and end with either "-edx.mongo", "-edx-analytics.sql",
"-edge.mongo", "-edge-analytics.sql". The first two indicate the data is for
an on-line course; the second two indicate that this is an edge course. There is
an on-line course and an edge course for each for each of the classes, but many 
of these are empty or nearly empty. 
	
7/28
course_enrollment.py, user.py, userprofile.py, certificates.py
	All modules that deal with basic file types now have a builddict function (taking
	the data file as supplied by edX and building a dictionary of those fields that
	are still of interest), a readdict (takes a csv file and reconstructs such a
	dictionary) and a writedict (takes a csv file and a dictionary, and writes the
	contents of the dictionary to the file).


3/17
user.py
	Removed the unused fields from the user object. It might makes sense to compress
	the csv files in a separate function that would read in the csv that was created
	by toCSV to remove these fields. This would require a change in the builddict 
	function, but would both save space and probably make everything faster.
	
course_enrollment.py
	Created a module containing an object that encapsulates the course enrollment data.
	This is a simple data file, containing the user id, the course id, and the date of
	enrollment. There is a function to scrub the csv file, and one to build a dictionary
	from the csv file. The internal id is not stored in the object.
	
convertfiles
	Added a package convertfiles, which contains the programs used to convert from xml to 
	csv. Placed toCSV.py, toCSVsmall.py, and xmltocsv.py into this package. Note that
	eclipse got funky at understanding that these modules now live within the package.
	
activity_log
	Added a package activity_log, containing the programs used to extract and manipulate
	the log files that contain all of the course activity. Placed combineLogbyTime.py,
	findEventTypes.py, and splitFiles.py into this package.
	
demographics
	Added a package demographics, containing the programs used to determine the demographics
	for a course. Moved getenrollmentdata,py, ph207demographcis.py, scrubstudentprofile.py, userprofile.py 
	to this package. 
	
demographics/ph207demographics
	Changed the name of the program to demographics.py, and added a first parameter that will
	indicate the name of the course for which the report (and the corresponding file) are
	prepared.

3/16
certificates.py
	Completed first implementation and documentation of the certificate file
	
userprofile.py
	Removed the lang and loc fields from the profile object. These have not been
	used or gathered since the prototype of 6.002x. 
	Added considerable pydoc to the defined functions.
	Moved to using logging rather than print() for error messages	

